from snakemake.utils import min_version
import os
import shutil
import numpy as np
import pandas as pd

##### set minimum snakemake version #####
min_version("6.0.0")

##### load rules ##########
include: "rules/common.smk"

workdir: config["outdir"]

samplesheet = pd.read_csv(
    config["samplesheet"],
    sep="\t",
    dtype={
        "sampleID": str,
        "condition": str,
        "group": str,
        "read1": str,
        "read2": str,
    },
    na_values=['', 'NA', 'na', 'N/A', 'n/a', 'NULL', 'null', 'None', 'none', '-'],
    skipinitialspace=True,
    keep_default_na=True
).set_index("sampleID")

# Strip whitespace from all string columns to handle user input issues
for col in ['condition', 'group', 'read1', 'read2']:
    samplesheet[col] = samplesheet[col].astype(str).str.strip()

# Enhanced NA detection for mixed single-end/paired-end support
def is_missing_read2(read2_value):
    """Check if read2 value indicates missing/absent file"""
    if pd.isna(read2_value):
        return True

    # Convert to string and normalize
    str_value = str(read2_value).strip().lower()

    # List of values that indicate missing read2
    missing_indicators = {
        '', 'na', 'n/a', 'null', 'none', '-',
        'nan', 'nil', 'empty', '0', 'false',
        'missing', 'absent', 'single', 'se'
    }

    return str_value in missing_indicators

# Apply enhanced detection
samplesheet['is_paired'] = ~samplesheet['read2'].apply(is_missing_read2)

# check the sample names are unique
assert len(np.unique(samplesheet.index.tolist())) == len(samplesheet.index.tolist()), "sampleIDs in the samplesheet are not unique"

# validate IgG control setup if enabled
if config["igg_control"]:
    groups = samplesheet["group"].unique()
    for group in groups:
        group_samples = samplesheet[samplesheet["group"] == group]
        controls = group_samples[group_samples["condition"] == "Control"]
        experiments = group_samples[group_samples["condition"] != "Control"]

        assert len(controls) == 1, f"Group '{group}' must have exactly one Control sample when igg_control=true, found {len(controls)}"
        assert len(experiments) >= 1, f"Group '{group}' must have at least one experimental sample when igg_control=true, found {len(experiments)}"

logdir = config["outdir"] + "/logs"
if not os.path.exists(logdir):
    os.makedirs(logdir)

# Sample classification for dual-rule architecture
def get_pe_samples():
    """Return list of paired-end sample IDs"""
    return samplesheet[samplesheet['is_paired']].index.tolist()

def get_se_samples():
    """Return list of single-end sample IDs"""
    return samplesheet[~samplesheet['is_paired']].index.tolist()

# Sample lists
SAMPLES = samplesheet.index.tolist()
EXPERIMENT = samplesheet[samplesheet["condition"] != "Control"].index.tolist()
CONTROL = samplesheet[samplesheet["condition"] == "Control"].index.tolist()
PE_SAMPLES = get_pe_samples()
SE_SAMPLES = get_se_samples()

# Enhanced validation for mixed samplesheets
def validate_mixed_samplesheet():
    """Comprehensive validation for mixed SE/PE samplesheets"""
    print("ðŸ”¬ Validating mixed single-end/paired-end samplesheet...")

    errors = []
    warnings = []

    for sample_id, row in samplesheet.iterrows():
        # Validate read1 (always required)
        if not os.path.exists(row['read1']):
            errors.append(f"Sample {sample_id}: Read1 file not found: {row['read1']}")

        # Validate read2 for paired-end samples
        if row['is_paired']:
            if pd.isna(row['read2']) or not os.path.exists(row['read2']):
                errors.append(f"Sample {sample_id}: Marked as paired-end but read2 file missing/invalid: {row['read2']}")
        else:
            if not pd.isna(row['read2']) and str(row['read2']).strip() not in ['', '-', 'NA', 'na', 'NULL', 'null', 'nan', 'NaN']:
                warnings.append(f"Sample {sample_id}: Marked as single-end but has read2 value: {row['read2']}")

    # Validate that at least one experimental sample exists
    if len(EXPERIMENT) == 0:
        errors.append("No experimental samples found. At least one non-Control sample is required for peak calling.")

    # Report results
    if errors:
        print("âŒ Samplesheet validation errors:")
        for error in errors:
            print(f"  {error}")
        raise ValueError("Samplesheet validation failed")

    if warnings:
        print("âš ï¸  Samplesheet validation warnings:")
        for warning in warnings:
            print(f"  {warning}")

    print(f"âœ… Samplesheet validation passed")
    print(f"   ðŸ“Š {len(PE_SAMPLES)} paired-end samples: {PE_SAMPLES}")
    print(f"   ðŸ“Š {len(SE_SAMPLES)} single-end samples: {SE_SAMPLES}")
    print(f"   ðŸ“ˆ Fragment analysis will be performed for {len(PE_SAMPLES)} paired-end samples only")

# Run validation
validate_mixed_samplesheet()

rule all:
    input:
        expand("qc/{sample}_1", sample=SAMPLES),
        expand("qc/{sample}_2", sample=SAMPLES),
        "stats/stats.txt",
        "stats/peaks_consolidated.txt",
        expand("stats/seacr/{sample}.heatmap.png", sample=EXPERIMENT),
        expand("stats/seacr/{sample}_gc_content.txt", sample=EXPERIMENT),
        expand("stats/macs3/{sample}_gc_content.txt", sample=EXPERIMENT),
        # Fragment analysis only for paired-end samples
        expand("alignments/{sample}.fraglengths.txt", sample=PE_SAMPLES),
        "stats/tss_enrichment.txt" if config["gtf_file"] else [],
        "reports/cutntag_analysis_report.html",


rule check_quality_pe:
    input:
        r1="trimmed/{sample}_val_1.fq.gz" if config["trim_adapters"] else r1_from_sample,
        r2="trimmed/{sample}_val_2.fq.gz" if config["trim_adapters"] else r2_from_sample,
    output:
        out1=directory("qc/{sample}_1"),
        out2=directory("qc/{sample}_2"),
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/check_quality_pe/{sample}.log",
    conda:
        "envs/fastqc.yaml"
    shell:
        """
        mkdir -p {output.out1} {output.out2}
        fastqc -o {output.out1} {input.r1}
        fastqc -o {output.out2} {input.r2}
        """

rule check_quality_se:
    input:
        r1="trimmed/{sample}_trimmed.fq.gz" if config["trim_adapters"] else r1_from_sample,
    output:
        out1=directory("qc/{sample}_1"),
        out2=directory("qc/{sample}_2"),
    wildcard_constraints:
        sample="|".join(SE_SAMPLES) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/check_quality_se/{sample}.log",
    conda:
        "envs/fastqc.yaml"
    shell:
        """
        mkdir -p {output.out1} {output.out2}
        fastqc -o {output.out1} {input.r1}
        echo "SINGLE_END_NO_R2_DATA" > {output.out2}/se_placeholder.txt
        """


rule trim_adapters_pe:
    input:
        r1=r1_from_sample,
        r2=r2_from_sample,
    output:
        out1="trimmed/{sample}_val_1.fq.gz",
        out2="trimmed/{sample}_val_2.fq.gz",
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/trim_adapters_pe/{sample}.log",
    conda:
        "envs/trim_galore.yaml"
    shell:
        """
        trim_galore --gzip --trim-n -o trimmed --basename {wildcards.sample} --paired {input.r1} {input.r2}
        """

rule trim_adapters_se:
    input:
        r1=r1_from_sample,
    output:
        out1="trimmed/{sample}_trimmed.fq.gz",
        out2="trimmed/{sample}_SE_placeholder.txt",
    wildcard_constraints:
        sample="|".join(SE_SAMPLES) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/trim_adapters_se/{sample}.log",
    conda:
        "envs/trim_galore.yaml"
    shell:
        """
        trim_galore --gzip --trim-n -o trimmed --basename {wildcards.sample} {input.r1}
        echo "SINGLE_END_NO_R2_TRIMMED" > {output.out2}
        """


rule align_reads_pe:
    input:
        ref=config["reference_fa"],
        r1="trimmed/{sample}_val_1.fq.gz" if config["trim_adapters"] else r1_from_sample,
        r2="trimmed/{sample}_val_2.fq.gz" if config["trim_adapters"] else r2_from_sample,
    output:
        sam=temp("alignments/{sample}.sam"),
        log="alignments/{sample}.alignments.txt",
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/align_reads_pe/{sample}.log",
    conda:
        "envs/bowtie.yaml"
    threads: 8
    params:
        aln="--local" if config["trim_adapters"] else "--end-to-end",
        ref=config["bt2_idx"],
    shell:
        """
        bowtie2 {params.aln} --very-sensitive --no-mixed --no-discordant --phred33 -I {config[fragment_min]} -X {config[fragment_max]} -p {threads} -x {params.ref} -1 {input.r1} -2 {input.r2} -S {output.sam} |& tee {output.log}
        """

rule align_reads_se:
    input:
        ref=config["reference_fa"],
        r1="trimmed/{sample}_trimmed.fq.gz" if config["trim_adapters"] else r1_from_sample,
    output:
        sam=temp("alignments/{sample}.sam"),
        log="alignments/{sample}.alignments.txt",
    wildcard_constraints:
        sample="|".join(SE_SAMPLES) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/align_reads_se/{sample}.log",
    conda:
        "envs/bowtie.yaml"
    threads: 8
    params:
        aln="--local" if config["trim_adapters"] else "--end-to-end",
        ref=config["bt2_idx"],
    shell:
        """
        bowtie2 {params.aln} --very-sensitive --phred33 -p {threads} -x {params.ref} -U {input.r1} -S {output.sam} |& tee {output.log}
        """


rule convert_to_binary:
    input:
        "alignments/{sample}.sam",
    output:
        bam="alignments/{sample}.sorted.bam",
    log:
        "logs/convert_to_binary/{sample}.log",
    conda:
        "envs/samtools.yaml"
    threads: 2
    shell:
        """
        samtools view -S -b {input} | samtools sort -@ {threads} -o {output.bam}
        """


rule index_bam:
    input:
        bam="alignments/{sample}.sorted.bam",
    output:
        bai="alignments/{sample}.sorted.bam.bai",
    log:
        "logs/index_bam/{sample}.log",
    conda:
        "envs/samtools.yaml"
    shell:
        """
        samtools index {input.bam}
        """


rule remove_duplicates:
    input:
        bam="alignments/{sample}.sorted.bam",
        bai="alignments/{sample}.sorted.bam.bai",
    output:
        bam="dedupalignments/{sample}.sorted.bam",
        metrics="dedupalignments/{sample}.rmdup.txt",
    log:
        "logs/remove_duplicates/{sample}.log",
    conda:
        "envs/picard.yaml"
    shell:
        """
        mkdir -p dedupalignments
        picard MarkDuplicates -I {input.bam} -O {output.bam} -METRICS_FILE {output.metrics} -REMOVE_DUPLICATES true
        """


use rule index_bam as index_dedup_bam with:
    input:
        bam="dedupalignments/{sample}.sorted.bam",
    output:
        bai="dedupalignments/{sample}.sorted.bam.bai",
    log:
        "logs/index_dedup_bam/{sample}.log",


rule find_mtdna_reads:
    input:
        bam="alignments/{sample}.sorted.bam",
        bai="alignments/{sample}.sorted.bam.bai",
    output:
        "alignments/{sample}.mtdna.txt",
    log:
        "logs/find_mtdna_reads/{sample}.log",
    conda:
        "envs/samtools.yaml"
    params:
        mtdna=config["mt_chrom"],
    shell:
        """
        samtools view -f0x40 {input.bam} {params.mtdna} | wc -l > {output}
        """


rule find_ecoli_reads_pe:
    input:
        ref=config["ecoli_reference"],
        r1="trimmed/{sample}_val_1.fq.gz" if config["trim_adapters"] else r1_from_sample,
        r2="trimmed/{sample}_val_2.fq.gz" if config["trim_adapters"] else r2_from_sample,
    output:
        sam=temp("alignments/{sample}.ecoli.sam"),
        txt="alignments/{sample}.ecoli.txt",
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/find_ecoli_reads_pe/{sample}.log",
    conda:
        "envs/bowtie.yaml"
    threads: 4
    params:
        ref=config["ecoli_bt2_idx"],
    shell:
        """
        bowtie2 --end-to-end --very-sensitive --no-mixed --no-discordant --phred33 -I {config[fragment_min]} -X {config[fragment_max]} -p {threads} -x {params.ref} -1 {input.r1} -2 {input.r2} -S {output.sam} &> {output.txt}
        """

rule find_ecoli_reads_se:
    input:
        ref=config["ecoli_reference"],
        r1="trimmed/{sample}_trimmed.fq.gz" if config["trim_adapters"] else r1_from_sample,
    output:
        sam=temp("alignments/{sample}.ecoli.sam"),
        txt="alignments/{sample}.ecoli.txt",
    wildcard_constraints:
        sample="|".join(SE_SAMPLES) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/find_ecoli_reads_se/{sample}.log",
    conda:
        "envs/bowtie.yaml"
    threads: 4
    params:
        ref=config["ecoli_bt2_idx"],
    shell:
        """
        bowtie2 --end-to-end --very-sensitive --phred33 -p {threads} -x {params.ref} -U {input.r1} -S {output.sam} &> {output.txt}
        """


rule report_on_alignments:
    input:
        aln=expand("alignments/{sample}.alignments.txt", sample=SAMPLES),
        mtn=expand("alignments/{sample}.mtdna.txt", sample=SAMPLES),
        dup=expand("dedupalignments/{sample}.rmdup.txt", sample=SAMPLES),
        eco=expand("alignments/{sample}.ecoli.txt", sample=SAMPLES),
    output:
        report("stats/stats.txt", caption="report/alignment.rst"),
    log:
        "logs/report_on_alignments/job.log",
    conda:
        "envs/R.yaml"
    script:
        "scripts/run_alignment_report.R"


rule fragment_size:
    input:
        "alignments/{sample}.sorted.bam",
    output:
        "alignments/{sample}.fraglengths.txt",
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/fragment_size/{sample}.log",
    conda:
        "envs/samtools.yaml"
    shell:
        """
        # Extract insert sizes for paired-end only
        samtools view -f 1 -F 0x04 {input} \
        | awk -F"\\t" 'function abs(x){{return ((x < 0.0) ? -x : x)}} {{print abs($9)}}' \
        | sort -n | uniq -c \
        | awk -v OFS="\\t" '{{print $2, $1/2}}' > {output}
        """




# we filter all reads overlapping the blacklist, remove non-primary and sex
# chromosomes and remove low-quality reads to create the bedgraph file which
# is given to the peak caller
rule bam_to_bed_pe:
    input:
        fai=config["reference_fai"],
        blklist=config["blacklist"],
        bai=(
            "dedupalignments/{sample}.sorted.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam.bai"
        ),
        bam=(
            "dedupalignments/{sample}.sorted.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam"
        ),
    output:
        bam=temp("{sample}.name.bam"),
        qbam=(
            "dedupalignments/{sample}.sorted.qflt.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam"
        ),
        qbai=(
            "dedupalignments/{sample}.sorted.qflt.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam.bai"
        ),
        bed=temp("bedalignments/{sample}.bed"),
        bedgraph="bedalignments/{sample}.bedgraph",
        binbed="bedalignments/{sample}.bin.bed",
    wildcard_constraints:
        sample="|".join(PE_SAMPLES) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/bam_to_bed_pe/{sample}.log",
    params:
        chroms=config["chromosome_file"],
        minq=config["minquality"],
    conda:
        "envs/bedtools.yaml"
    threads: 2
    shell:
        """
        # Paired-end processing: use -f 1 flag for paired reads
        samtools view -h -q {params.minq} -f 1 -F 3852 {input.bam} $(tr '\\n' ' ' < {params.chroms}) \
        | awk -F "\\t" '/^@/ {{print; next}} ($7=="=" && $9 <= 1000 && $9 >= -1000) {{print}}' \
        | samtools view -b \
        | bedtools intersect -abam - -b {input.blklist} -v \
        > {output.qbam}

        samtools index {output.qbam}
        samtools sort -@ {threads} -n -O BAM -o {output.bam} {output.qbam}

        # Convert to BEDPE format for paired-end
        bedtools bamtobed -i {output.bam} -bedpe \
        | cut -f 1,2,6 \
        | sort -k1,1 -k2,2n -k3,3n \
        > {output.bed}

        # Common processing
        bedtools genomecov -bg -i {output.bed} \
            -g <(cut -f 1,2 {input.fai}) > {output.bedgraph}

        awk -v w=500 '{{print $1, int(($2 + $3)/(2*w))*w + w/2}}' {output.bed} \
        | sort -k1,1V -k2,2n \
        | uniq -c \
        | awk -v OFS="\\t" '{{print $2, $3, $1}}' \
        | sort -k1,1V -k2,2n > {output.binbed}
        """

rule bam_to_bed_se:
    input:
        fai=config["reference_fai"],
        blklist=config["blacklist"],
        bai=(
            "dedupalignments/{sample}.sorted.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam.bai"
        ),
        bam=(
            "dedupalignments/{sample}.sorted.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam"
        ),
    output:
        bam=temp("{sample}.name.bam"),
        qbam=(
            "dedupalignments/{sample}.sorted.qflt.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam"
        ),
        qbai=(
            "dedupalignments/{sample}.sorted.qflt.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam.bai"
        ),
        bed=temp("bedalignments/{sample}.bed"),
        bedgraph="bedalignments/{sample}.bedgraph",
        binbed="bedalignments/{sample}.bin.bed",
    wildcard_constraints:
        sample="|".join(SE_SAMPLES) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/bam_to_bed_se/{sample}.log",
    params:
        chroms=config["chromosome_file"],
        minq=config["minquality"],
    conda:
        "envs/bedtools.yaml"
    threads: 2
    shell:
        """
        # Single-end processing: use -F 3844 (remove paired-specific flags)
        samtools view -h -q {params.minq} -F 3844 {input.bam} $(tr '\\n' ' ' < {params.chroms}) \
        | samtools view -b \
        | bedtools intersect -abam - -b {input.blklist} -v \
        > {output.qbam}

        samtools index {output.qbam}
        samtools sort -@ {threads} -n -O BAM -o {output.bam} {output.qbam}

        # Convert to BED format for single-end
        bedtools bamtobed -i {output.bam} \
        | sort -k1,1 -k2,2n -k3,3n \
        > {output.bed}

        # Common processing
        bedtools genomecov -bg -i {output.bed} \
            -g <(cut -f 1,2 {input.fai}) > {output.bedgraph}

        awk -v w=500 '{{print $1, int(($2 + $3)/(2*w))*w + w/2}}' {output.bed} \
        | sort -k1,1V -k2,2n \
        | uniq -c \
        | awk -v OFS="\\t" '{{print $2, $3, $1}}' \
        | sort -k1,1V -k2,2n > {output.binbed}
        """

rule calculate_seacr_peak_gc:
    input:
        peaks="peaks/seacr/{sample}.peaks.stringent.bed",
        fasta=config["reference_fa"],
    output:
        gc_stats="stats/seacr/{sample}_gc_content.txt",
    wildcard_constraints:
        sample="|".join(EXPERIMENT) if EXPERIMENT else "NO_EXPERIMENT_SAMPLES"
    log:
        "logs/calculate_peak_gc/seacr_{sample}.log",
    conda:
        "envs/bedtools.yaml"
    shell:
        """
        if [ -s {input.peaks} ]; then
            bedtools nuc -fi {input.fasta} -bed {input.peaks} | \
            awk 'NR>1 {{print $8}}' | \
            sort -n | \
            awk '{{a[NR]=$1}} END {{n=NR; if(n==0) print "{wildcards.sample}", 0; else if(n%2==1) print "{wildcards.sample}", a[(n+1)/2]; else print "{wildcards.sample}", (a[n/2]+a[n/2+1])/2}}' \
            > {output.gc_stats}
        else
            echo "{wildcards.sample} 0" > {output.gc_stats}
        fi
        """

rule calculate_macs_peak_gc:
    input:
        peaks="peaks/macs3/{sample}_peaks.narrowPeak",
        fasta=config["reference_fa"],
    output:
        gc_stats="stats/macs3/{sample}_gc_content.txt",
    wildcard_constraints:
        sample="|".join(EXPERIMENT) if EXPERIMENT else "NO_EXPERIMENT_SAMPLES"
    log:
        "logs/calculate_peak_gc/macs3_{sample}.log",
    conda:
        "envs/bedtools.yaml"
    shell:
        """
        if [ -s {input.peaks} ]; then
            bedtools nuc -fi {input.fasta} -bed {input.peaks} | \
            awk 'NR>1 {{print $12}}' | \
            sort -n | \
            awk '{{a[NR]=$1}} END {{n=NR; if(n==0) print "{wildcards.sample}", 0; else if(n%2==1) print "{wildcards.sample}", a[(n+1)/2]; else print "{wildcards.sample}", (a[n/2]+a[n/2+1])/2}}' \
            > {output.gc_stats}
        else
            echo "{wildcards.sample} 0" > {output.gc_stats}
        fi
        """


rule call_seacr_peaks:
    input:
        con=get_bedg_control if config["igg_control"] else [],
        exp="bedalignments/{sample}.bedgraph",
    output:
        "peaks/seacr/{sample}.peaks.stringent.bed",
    wildcard_constraints:
        sample="|".join(EXPERIMENT) if EXPERIMENT else "NO_EXPERIMENT_SAMPLES"
    log:
        "logs/call_seacr_peaks/{sample}.log",
    conda:
        "envs/seacr.yaml"
    params:
        prefix="peaks/seacr/{sample}.peaks",
        has_igg=config["igg_control"],
    shell:
        """
        if [ {params.has_igg} == True ]; then
            bash `which SEACR_1.3.sh` {input.exp} {input.con} norm stringent {params.prefix} || true
        else
            bash `which SEACR_1.3.sh` {input.exp} {config[seacr_qvalue]} norm stringent {params.prefix} || true
        fi
        if [ ! -f {output} ]; then
            touch {output}
        fi
        """

rule call_macs_peaks_pe:
    input:
        con=get_bam_control if config["igg_control"] else [],
        exp=(
            "dedupalignments/{sample}.sorted.qflt.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam"
        ),
        expi=(
            "dedupalignments/{sample}.sorted.qflt.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam.bai"
        ),
    output:
        sbed="peaks/macs3/{sample}_summits.bed",
        npeaks="peaks/macs3/{sample}_peaks.narrowPeak",
        peaks="peaks/macs3/{sample}_peaks.xls"
    wildcard_constraints:
        sample="|".join([s for s in PE_SAMPLES if s in EXPERIMENT]) if PE_SAMPLES else "NO_PE_SAMPLES"
    log:
        "logs/call_macs_peaks_pe/{sample}.log",
    conda:
        "envs/macs.yaml"
    params:
        prefix="{sample}",
        dir="peaks/macs3",
        has_igg=config["igg_control"],
    shell:
        """
        if [ {params.has_igg} == True ]; then
           macs3 callpeak -t {input.exp} -c {input.con} -f BAMPE -g hs -n "{params.prefix}" --outdir {params.dir} -B -q {config[macs3_qvalue_with_control]} || true
        else
           macs3 callpeak -t {input.exp} -f BAMPE -g hs -n "{params.prefix}" --outdir {params.dir} -B -q {config[macs3_qvalue_no_control]} --nolambda || true
        fi
        if [ ! -f {output.npeaks} ]; then
            touch {output.sbed}
            touch {output.npeaks}
            touch {output.peaks}
        fi
        """

rule call_macs_peaks_se:
    input:
        con=get_bam_control if config["igg_control"] else [],
        exp=(
            "dedupalignments/{sample}.sorted.qflt.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam"
        ),
        expi=(
            "dedupalignments/{sample}.sorted.qflt.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.qflt.bam.bai"
        ),
    output:
        sbed="peaks/macs3/{sample}_summits.bed",
        npeaks="peaks/macs3/{sample}_peaks.narrowPeak",
        peaks="peaks/macs3/{sample}_peaks.xls"
    wildcard_constraints:
        sample="|".join([s for s in SE_SAMPLES if s in EXPERIMENT]) if SE_SAMPLES else "NO_SE_SAMPLES"
    log:
        "logs/call_macs_peaks_se/{sample}.log",
    conda:
        "envs/macs.yaml"
    params:
        prefix="{sample}",
        dir="peaks/macs3",
        has_igg=config["igg_control"],
    shell:
        """
        if [ {params.has_igg} == True ]; then
           macs3 callpeak -t {input.exp} -c {input.con} -f BAM -g hs -n "{params.prefix}" --outdir {params.dir} -B -q {config[macs3_qvalue_with_control]} || true
        else
           macs3 callpeak -t {input.exp} -f BAM -g hs -n "{params.prefix}" --outdir {params.dir} -B -q {config[macs3_qvalue_no_control]} --nolambda || true
        fi
        if [ ! -f {output.npeaks} ]; then
            touch {output.sbed}
            touch {output.npeaks}
            touch {output.peaks}
        fi
        """

rule assess_peaks:
    input:
        peaks=expand("peaks/seacr/{sample}.peaks.stringent.bed", sample=EXPERIMENT),
        stats="stats/stats.txt",
        gc_files=expand("stats/seacr/{sample}_gc_content.txt", sample=EXPERIMENT),
    output:
        peakN="stats/seacr/peaks_num.txt",
        peakR="stats/seacr/peaks_reproducibility.txt",
        peakF="stats/seacr/peaks_frip.txt",
        peakW="stats/seacr/peaks_width_stats.txt",
        peakG="stats/seacr/peaks_gc_content.txt",
    log:
        "logs/assess_seacr_peaks/job.log",
    conda:
        "envs/R.yaml"
    params:
        samplesheet=config["samplesheet"],
        subdir="dedupalignments" if config["dedup"] else "alignments",
        method="SEACR"
    script:
        "scripts/assess_peaks.R"

use rule assess_peaks as assess_macs_peaks with:
    input:
        peaks=expand("peaks/macs3/{sample}_peaks.narrowPeak", sample=EXPERIMENT),
        stats="stats/stats.txt",
        gc_files=expand("stats/macs3/{sample}_gc_content.txt", sample=EXPERIMENT),
    output:
        peakN="stats/macs3/peaks_num.txt",
        peakR="stats/macs3/peaks_reproducibility.txt",
        peakF="stats/macs3/peaks_frip.txt",
        peakW="stats/macs3/peaks_width_stats.txt",
        peakG="stats/macs3/peaks_gc_content.txt",
    log:
        "logs/assess_macs_peaks/job.log",
    params:
        samplesheet=config["samplesheet"],
        subdir="dedupalignments" if config["dedup"] else "alignments",
        method="MACS"

rule consolidate_stats:
    input:
        seacr_num="stats/seacr/peaks_num.txt",
        seacr_reprod="stats/seacr/peaks_reproducibility.txt",
        seacr_frip="stats/seacr/peaks_frip.txt",
        seacr_width="stats/seacr/peaks_width_stats.txt",
        seacr_gc="stats/seacr/peaks_gc_content.txt",
        macs3_num="stats/macs3/peaks_num.txt",
        macs3_reprod="stats/macs3/peaks_reproducibility.txt",
        macs3_frip="stats/macs3/peaks_frip.txt",
        macs3_width="stats/macs3/peaks_width_stats.txt",
        macs3_gc="stats/macs3/peaks_gc_content.txt",
    output:
        consolidated="stats/peaks_consolidated.txt",
    log:
        "logs/consolidate_stats/job.log",
    conda:
        "envs/R.yaml"
    script:
        "scripts/consolidate_stats.R"

rule make_peak_heatmap:
    input:
        bam=(
            "dedupalignments/{sample}.sorted.bam"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam"
        ),
        bai=(
            "dedupalignments/{sample}.sorted.bam.bai"
            if config["dedup"]
            else "alignments/{sample}.sorted.bam.bai"
        ),
        peaks="peaks/seacr/{sample}.peaks.stringent.bed",
    output:
        bw="alignments/{sample}_raw.bw",
        peaks=temp("peaks/seacr/{sample}.peaks.summitRegion.bed"),
        mat=temp("peaks/seacr/{sample}.mat.gz"),
        heatmap="stats/seacr/{sample}.heatmap.png",
    wildcard_constraints:
        sample="|".join(EXPERIMENT) if EXPERIMENT else "NO_EXPERIMENT_SAMPLES"
    log:
        "logs/get_coverage/{sample}.log",
    conda:
        "envs/deeptools.yaml"
    threads: 2
    shell:
        """
        bamCoverage -b {input.bam} -o {output.bw}

        if [ -s {input.peaks} ]; then 
            awk '{{split($6, summit, ":"); split(summit[2], region, "-"); print summit[1]"\t"region[1]"\t"region[2]}}' {input.peaks} > {output.peaks} 

            computeMatrix reference-point \
              -S {output.bw} \
              -R {output.peaks} \
              --skipZeros \
              -o {output.mat} -p {threads} -a {config[heatmap_window]} -b {config[heatmap_window]} \
              --referencePoint center

            plotHeatmap -m {output.mat} -out {output.heatmap} --sortUsing sum \
            --startLabel "Peak Start" --endLabel "Peak End" \
            --xAxisLabel "" \
            --regionsLabel "Peaks" --samplesLabel "{wildcards.sample}"
        else
            touch {output.peaks}
            touch {output.mat}
            touch {output.heatmap} 
        fi
        """

if config["gtf_file"]:
    rule extract_tss:
        input:
            gtf=config["gtf_file"],
        output:
            tss="annotations/tss.bed",
        log:
            "logs/extract_tss/job.log",
        conda:
            "envs/bedtools.yaml"
        shell:
            """
            mkdir -p annotations

            # Handle both gzipped and regular GTF files
            if [[ {input.gtf} == *.gz ]]; then
                zcat {input.gtf} | awk '$3=="gene" && $7=="+" && $0~/gene_type "protein_coding"/ {{print $1"\\t"($4-1)"\\t"$4"\\t"$10"\\t"$6"\\t"$7}} $3=="gene" && $7=="-" && $0~/gene_type "protein_coding"/ {{print $1"\\t"($5-1)"\\t"$5"\\t"$10"\\t"$6"\\t"$7}}' | sed 's/[";]//g' > {output.tss}
            else
                awk '$3=="gene" && $7=="+" && $0~/gene_type "protein_coding"/ {{print $1"\\t"($4-1)"\\t"$4"\\t"$10"\\t"$6"\\t"$7}} $3=="gene" && $7=="-" && $0~/gene_type "protein_coding"/ {{print $1"\\t"($5-1)"\\t"$5"\\t"$10"\\t"$6"\\t"$7}}' | sed 's/[";]//g' > {output.tss}
            fi
            """

    rule calculate_tss_enrichment:
        input:
            tss="annotations/tss.bed",
            bw=expand("alignments/{sample}_raw.bw", sample=SAMPLES),
        output:
            matrix=protected("stats/tss_matrix.gz"),
            enrichment=protected("stats/tss_enrichment.txt"),
            plot="stats/tss_enrichment.pdf",
        log:
            "logs/calculate_tss_enrichment/job.log",
        conda:
            "envs/deeptools.yaml"
        threads: 4
        shell:
            """
            computeMatrix reference-point \\
                -S {input.bw} \\
                -R {input.tss} \\
                --referencePoint center \\
                -a 2000 -b 2000 \\
                --binSize 50 \\
                --skipZeros \\
                -o {output.matrix} \\
                -p {threads}

            plotProfile -m {output.matrix} \\
                -out {output.plot} \\
                --averageType mean \\
                --plotTitle "TSS Enrichment Profile" \\
                --yAxisLabel "Mean Coverage" 

            python3 {workflow.basedir}/scripts/calculate_tss_enrichment.py {output.matrix} {output.enrichment}
            """

rule generate_html_report:
    input:
        stats="stats/stats.txt",
        consolidated="stats/peaks_consolidated.txt",
        seacr_num="stats/seacr/peaks_num.txt",
        seacr_reprod="stats/seacr/peaks_reproducibility.txt",
        seacr_frip="stats/seacr/peaks_frip.txt",
        seacr_width="stats/seacr/peaks_width_stats.txt",
        seacr_gc="stats/seacr/peaks_gc_content.txt",
        macs3_num="stats/macs3/peaks_num.txt",
        macs3_reprod="stats/macs3/peaks_reproducibility.txt",
        macs3_frip="stats/macs3/peaks_frip.txt",
        macs3_width="stats/macs3/peaks_width_stats.txt",
        macs3_gc="stats/macs3/peaks_gc_content.txt",
        seacr_peaks=expand("peaks/seacr/{sample}.peaks.stringent.bed", sample=EXPERIMENT),
        macs3_peaks=expand("peaks/macs3/{sample}_peaks.narrowPeak", sample=EXPERIMENT),
        fragments=expand("alignments/{sample}.fraglengths.txt", sample=PE_SAMPLES),
        coverage_bins=expand("bedalignments/{sample}.bin.bed", sample=SAMPLES),
        tss_enrichment="stats/tss_enrichment.txt" if config["gtf_file"] else [],
    output:
        html_report="reports/cutntag_analysis_report.html",
    log:
        "logs/generate_html_report/job.log",
    conda:
        "envs/R.yaml"
    params:
        samplesheet=config["samplesheet"],
        outdir=config["outdir"],
        snakefile_dir=workflow.basedir,
    script:
        "scripts/render_html_report.R"

# Rule ordering to prevent conflicts when both PE and SE samples are present
ruleorder: check_quality_pe > check_quality_se
ruleorder: trim_adapters_pe > trim_adapters_se
ruleorder: align_reads_pe > align_reads_se
ruleorder: find_ecoli_reads_pe > find_ecoli_reads_se
ruleorder: bam_to_bed_pe > bam_to_bed_se
ruleorder: call_macs_peaks_pe > call_macs_peaks_se

onsuccess:
    shutil.rmtree(os.path.join(config["outdir"], "alignments"), ignore_errors=True)

